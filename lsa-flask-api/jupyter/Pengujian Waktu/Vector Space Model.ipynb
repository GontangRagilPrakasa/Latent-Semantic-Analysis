{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = ['confidence bands for roc curves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Waktu Proses  0.0009987354278564453  Detik.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer # tf-idf\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords # preprocessing\\\n",
    "from math import*\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "class Engine:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cosine_score = []\n",
    "        self.train_set = []  # Documents\n",
    "        self.test_set = []  # Query\n",
    "\n",
    "    def addDocument(self, word): # fungsi untuk menambahkan dokumen dataset ke dalam list train_set\n",
    "        self.train_set.append(word)\n",
    "\n",
    "    def setQuery(self, word):  # fungsi untuk menambahkan data query ke dalam list test_Set\n",
    "        self.test_set.append(word)\n",
    "\n",
    "    def process_score(self):\n",
    "        stopWords = stopwords.words('english') \n",
    "        vectorizer = CountVectorizer()\n",
    "\n",
    "        transformer = TfidfTransformer()\n",
    "\n",
    "        trainVectorizerArray = vectorizer.fit_transform(self.train_set).toarray() \n",
    "        # menghitung Bobot dokumen dataset dan uji dan kemudian disimpan dalam bentuk array \n",
    "        testVectorizerArray = vectorizer.transform(self.test_set).toarray()\n",
    "\n",
    "        cx = lambda a, b: round(np.inner(a, b) / (LA.norm(a) * LA.norm(b)), 3) \n",
    "        #fungsi tanpa nama untuk normalisasi data dan definisi rumus Cosine Similarity \n",
    "        #         print testVectorizerArray\n",
    "        output = []\n",
    "        for i in range(0, len(testVectorizerArray)):\n",
    "            output.append([])\n",
    "\n",
    "        for vector in trainVectorizerArray:\n",
    "            # print vector\n",
    "            u = 0\n",
    "            for testV in testVectorizerArray:\n",
    "                #perhitungan Cosine Similarity dalam bentuk vector dari dataset dengan query\n",
    "                #yang di masukan yang kemudian mengembalikan nilai cosine ke dalam variable\n",
    "                #cosine_score dalam bentuk list.\n",
    "                # print testV\n",
    "                cosine = cx(vector, testV)\n",
    "                #                 self.cosine_score.append(cosine)\n",
    "                #                 bulatin = (round(cosine),2)\n",
    "                output[u].append((cosine))\n",
    "                u = u + 1\n",
    "        return output\n",
    "        # return testVectorizerArray\n",
    "import pandas as pd\n",
    "setD = pd.read_excel('preprocessed-dataset.xlsx',sep=',')\n",
    "#print(setD)\n",
    "Doc = setD['preprocessed_judul'].astype('str')\n",
    "#print(Doc)\n",
    "# kunci = pd.read_excel('query.xlsx',sep=',')\n",
    "# query = kunci['judul'].astype('str')\n",
    "\n",
    "\n",
    "engine = Engine()\n",
    "\n",
    "docs = [str(x) for x in Doc]\n",
    "documentNames = list()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    engine.addDocument(doc) \n",
    "    documentNames.append(\"Document_{}\".format(i+1))\n",
    "for queries in query:\n",
    "    engine.setQuery(queries) #inputandata uji\n",
    "\n",
    "titles_score = engine.process_score()\n",
    "ScoreDf = (pd.DataFrame(titles_score)).T\n",
    "ScoreDf.columns = query\n",
    "ScoreDf[\"Documents\"] = documentNames\n",
    "ScoreDf[\"Abstrak\"] = setD[\"preprocessed_abstrak\"].values\n",
    "ScoreDf\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "#svm_ = svm.SVC(kernel='linear')\n",
    "df_listed = list()\n",
    "for i in query:\n",
    "    labels = list()\n",
    "    #labelss = list()\n",
    "    for j in ScoreDf[i]:\n",
    "        if j>0.000:\n",
    "            labels.append(1)\n",
    "            #labelss.append(cross_val_score(svm_, query, Doc, cv=10)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "            #labelss.append(cross_val_score(svm_, query, Doc, cv=10)\n",
    "    datadf = pd.DataFrame(ScoreDf[i])\n",
    "    datadf['Documents'] = ScoreDf['Documents']\n",
    "    datadf['Labels'] = labels\n",
    "    datadf['abstrak'] = ScoreDf['Abstrak'].values\n",
    "    datadf['reviewer'] = setD['Reviewer'].values\n",
    "    datadf['Judul'] = setD['Judul'].values\n",
    "    df_listed.append(datadf.sort_values(by=[i], ascending=False))\n",
    "#df_listed\n",
    "#df_listed[0].head(5)\n",
    "\n",
    "#import urllib2  \n",
    "import time  \n",
    "awal = time.time()\n",
    "df_listed[0].head(10) \n",
    "akhir = time.time()  \n",
    "print (\"Total Waktu Proses \", akhir- awal, \" Detik.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Waktu Proses  0.0009980201721191406  Detik.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer # tf-idf\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords # preprocessing\\\n",
    "import math\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "class Engine:\n",
    "    def __init__(self):\n",
    "        self.cosine_score = []\n",
    "        self.train_set = []  # Documents\n",
    "        self.test_set = []  # Query\n",
    "\n",
    "    def addDocument(self, word): # fungsi untuk menambahkan dokumen dataset ke dalam list train_set\n",
    "        self.train_set.append(word)\n",
    "\n",
    "    def setQuery(self, word):  # fungsi untuk menambahkan data query ke dalam list test_Set\n",
    "        self.test_set.append(word)\n",
    "\n",
    "    def process_score(self):\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, max_df = 0.5, smooth_idf=True)\n",
    "        svd_model = TruncatedSVD(n_components=100,algorithm='randomized',n_iter=10)\n",
    "        lsa = Pipeline([('tfidf', vectorizer),('svd', svd_model)])\n",
    "        #print(lsa)\n",
    "        #transformer = TfidfTransformer(stop_words='english', use_idf=True, smooth_idf=True)\n",
    "\n",
    "        trainVectorizerArray = lsa.fit_transform(self.train_set).tolist() \n",
    "        # menghitung Bobot dokumen dataset dan uji dan kemudian disimpan dalam bentuk array \n",
    "        testVectorizerArray = lsa.transform(self.test_set).tolist()\n",
    "\n",
    "        cx = lambda a, b: round(np.inner(a, b) / (LA.norm(a) * LA.norm(b)), 3) \n",
    "        #fungsi tanpa nama untuk normalisasi data dan definisi rumus Cosine Similarity \n",
    "        #print (testVectorizerArray)\n",
    "        output = []\n",
    "        for i in range(0, len(testVectorizerArray)):\n",
    "            output.append([])\n",
    "\n",
    "        for vector in trainVectorizerArray:\n",
    "            #print (vector)\n",
    "            u = 0\n",
    "            for testV in testVectorizerArray:\n",
    "                #perhitungan Cosine Similarity dalam bentuk vector dari dataset dengan query\n",
    "                #yang di masukan yang kemudian mengembalikan nilai cosine ke dalam variable\n",
    "                #cosine_score dalam bentuk list.\n",
    "                #print (testV)\n",
    "                cosine = cx(vector, testV)\n",
    "                #print(cosine)\n",
    "                #                 self.cosine_score.append(cosine)\n",
    "                #                 bulatin = (round(cosine),2)\n",
    "                if math.isnan(cosine):\n",
    "                    cosine = 0\n",
    "                output[u].append((cosine))\n",
    "                u = u + 1\n",
    "        return output\n",
    "        #return testVectorizerArray\n",
    "        \n",
    "import pandas as pd\n",
    "setD = pd.read_excel('preprocessed-dataset.xlsx',sep=',')\n",
    "#print(setD)\n",
    "Doc = setD['preprocessed_judul'].astype('str')\n",
    "#print(Doc)\n",
    "# kunci = pd.read_excel('query.xlsx',sep=',')\n",
    "# query = kunci['judul'].astype('str')\n",
    "\n",
    "        \n",
    "engine = Engine()\n",
    "\n",
    "docs = [str(x) for x in Doc]\n",
    "documentNames = list()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    engine.addDocument(doc) \n",
    "    documentNames.append(\"Document_{}\".format(i+1))\n",
    "for queries in query:\n",
    "    engine.setQuery(queries) #inputandata uji\n",
    "\n",
    "titles_score = engine.process_score()\n",
    "ScoreDf = (pd.DataFrame(titles_score)).T\n",
    "ScoreDf.columns = query\n",
    "ScoreDf[\"Documents\"] = documentNames\n",
    "ScoreDf[\"Abstrak\"] = setD[\"preprocessed_abstrak\"].values\n",
    "ScoreDf\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "#svm_ = svm.SVC(kernel='linear')\n",
    "df_listed = list()\n",
    "for i in query:\n",
    "    labels = list()\n",
    "    #labelss = list()\n",
    "    for j in ScoreDf[i]:\n",
    "        if j>0.000:\n",
    "            labels.append(1)\n",
    "            #labelss.append(cross_val_score(svm_, query, Doc, cv=10)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "            #labelss.append(cross_val_score(svm_, query, Doc, cv=10)\n",
    "    datadf = pd.DataFrame(ScoreDf[i])\n",
    "    datadf['Documents'] = ScoreDf['Documents']\n",
    "    datadf['Labels'] = labels\n",
    "    datadf['abstrak'] = ScoreDf['Abstrak'].values\n",
    "    datadf['reviewer'] = setD['Reviewer'].values\n",
    "    datadf['Judul'] = setD['Judul'].values\n",
    "    df_listed.append(datadf.sort_values(by=[i], ascending=False))\n",
    "#df_listed\n",
    "#df_listed[0].head(5)\n",
    "#import urllib2  \n",
    "import time  \n",
    "awal = time.time()\n",
    "df_listed[0].head(10) \n",
    "akhir = time.time()  \n",
    "print (\"Total Waktu Proses \", akhir- awal, \" Detik.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
